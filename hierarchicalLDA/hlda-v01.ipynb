{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import dirichlet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "class DocTermMatrix(object):\n",
    "    def __init__(self, corpus, **kwargs):\n",
    "        '''\n",
    "        corpus: a collection of raw documents\n",
    "        with **kwargs you can pass arguments of CountVectorizer\n",
    "        creates a doc-term matrix instance (as scipy sparse matrix)\n",
    "        '''\n",
    "\n",
    "        vectorizer = CountVectorizer(**kwargs)\n",
    "        self.num_docs = len(corpus)\n",
    "        self.vocab_size = len(vectorizer.vocabulary_)\n",
    "        self.shape = (self.num_docs, self.vocab_size)\n",
    "        self.matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    def dataframe(self):\n",
    "        '''\n",
    "        returns the doc-term matrix as pandas dataframe\n",
    "        '''\n",
    "        return pd.DataFrame(x1.toarray(), columns = vectorizer.get_feature_names())\n",
    "\n",
    "    def length(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of words in each document, i.e. its length.\n",
    "        Returns an array of the same size as the corpus.\n",
    "        \"\"\"\n",
    "        # lengths_list = np.zeros(self.row_dim)\n",
    "        lengths_list = self.df().apply(lambda x: len(x.nonzero()[0]), axis=1)\n",
    "        return lengths_list\n",
    "\n",
    "    @classmethod\n",
    "    def word_index(cls, doc_row):\n",
    "        \"\"\"\n",
    "        doc_row: document vector (of size V, vocab-size)\n",
    "        Returns an array of repeated indices/terms for the words in the document;\n",
    "        the indices/terms are repeated the same number of times as the number of occurrences for the corresponding word;\n",
    "        the length of the array is thus equal to the document length.\n",
    "        \"\"\"\n",
    "        # doc_row = np.array(doc_row)\n",
    "        for idx in doc_row.nonzero()[0]:\n",
    "            for i in range(int(doc_row[idx])):\n",
    "                yield idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My journey through hLDA:\n",
    "\n",
    "Below, firstly, is my implementation of the hierarchical prior for the collapsed Gibbs sampler, largely following the notation of the notes.\n",
    "\n",
    "I have my questions in the code, but here is an overarching one:\n",
    "    - I can understand how, by implementing the Polya urn scheme with probabilities that reward previously drawn topics (\"rich get richer\"?), we would get a few topics being general and others more specific. It is not obvious to me where we made sure that there is a single, unique topic at the root of every path through the hierarchy. [which is how it is plotted in Blei et al. 2004 - 'Hierarchical Topic Models and the Nested Chinese Restaurant Process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hLDA(object):\n",
    "    def __init__(self, tree_depth, alpha, gamma, eta):\n",
    "        '''\n",
    "        gamma: the parameter for the (base) Dirichlet from which the prior vector ('m') is drawn.\n",
    "                in the notes' notation, this is alpha'.\n",
    "        tree_depth: a parameter for the number of levels in a hierarchy.\n",
    "        '''\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.k = tree_depth\n",
    "        self.eta = eta\n",
    "\n",
    "    def priors(self, dtm, seed=None):\n",
    "        self.num_docs, self.vocab_size = dtm.shape\n",
    "\n",
    "        # number of times topic K allocaition variables generate term/word V\n",
    "        self.nkv = np.zeros((self.num_topics, self.vocab_size))\n",
    "\n",
    "        #### dtm = DocTermMatrix(corpus).dataframe()\n",
    "        # Document-specific vectors that record the multiplicity of each label\n",
    "        self.local_count = np.zeros((self.num_docs, self.k))\n",
    "        # Global vector which keeps tracks of all draws from the base measure\n",
    "        self.global_count = np.zeros(self.k)\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Simulating from the hierarchical prior for the first document\n",
    "        sample = {}\n",
    "        sample[0] = multinomial(1,[1/self.k]*self.k).nonzero()[0][0] # sample s1\n",
    "        R = set(sample.values())\n",
    "        M = 1\t\n",
    "        z = {}\n",
    "        # set the topic of the first word in the first doc to s1\n",
    "        z[(0,0)] = sample[0]\n",
    "        self.global_count[0, sample[0]] += 1\n",
    "        self.local_—Åount[0, sample[0]] += 1\n",
    "        for n, w in enumerate(DocTermMatrix.word_index(dtm[0,:])):\n",
    "            draw = hLDA.roll_die_first_doc()\n",
    "            if draw is not None and draw <= self.k:\n",
    "                z[(0,n+1)] = draw\n",
    "                self.local_count[0, draw] += 1\n",
    "            elif draw > self.k and draw <= 2*self.k:\n",
    "                sample[M+1] = draw - self.k # assign the s_{Mn+1} = s\n",
    "\n",
    "                # update local and global count?\n",
    "                self.local_count[0, sample[M+1]] += 1\n",
    "                self.global_count[0, sample[M+1]] += 1\n",
    "\n",
    "                M += 1\n",
    "                # set z_{n+1} equal to?\n",
    "                z[(0, n+1)] = sample[M+1]\n",
    "            elif draw > 2*self.k:\n",
    "                new_s = multinomial(1,[1/self.k]*self.k).nonzero()[0][0]\n",
    "                sample[M+1] = new_s\n",
    "                M += 1\n",
    "                self.global_count[0, new_s] += 1\n",
    "                self.local_count[0, new_s] += 1\n",
    "                # set z_{n+1} equal to?\n",
    "                z[(0, n+1)] = new_s\n",
    "            R = set(sample)\n",
    "\n",
    "        # hierarchical prior across documnents\n",
    "        for idx, bow in dtm[1:].iterrows(): # bow = bag-of-words\n",
    "            for n, w in enumerate(DocTermMatrix.word_index(bow)):\n",
    "                if n == 0:\n",
    "                    draw = hLDA.roll_die_first_word()\n",
    "                    if draw <= self.k:\n",
    "                        z[(idx,n)] = draw # double-check\n",
    "                        self.local_count[idx, draw] = 1\n",
    "        # do we increment counts for M_{k,v} here too? how does this affect the prior of beta vs. usual case?\n",
    "                        self.nkv[z[(idx,n)],w] += 1\n",
    "                    else:\n",
    "                        sample[M+1] = multinomial(1,[1/self.k]*self.k).nonzero()[0][0]\n",
    "                        z[(idx,n)] = sample[M+1] # is this the s in the notes?\n",
    "                        M += 1\n",
    "                        self.nkv[z[(d,i)],w] += 1\n",
    "                else:\n",
    "                    draw = hLDA.roll_die_nth_word()\n",
    "                    if draw <= self.k:\n",
    "                        # need to align n's\n",
    "                        z[(idx, n+1)] = draw\n",
    "                        self.global_count[idx, draw] += 1\n",
    "                        self.local_count[idx, draw] += 1\n",
    "                        self.nkv[z[(idx,n+1)],w] += 1\n",
    "\n",
    "                    else:\n",
    "                        sample[M+1] = multinomial(1,[1/self.k]*self.k).nonzero()[0][0]\n",
    "                        z[(idx,n+1)] = sample[M+1]\n",
    "                        self.global_count[idx, sample[M+1]] += 1\n",
    "                        self.local_count[idx, sample[M+1]] += 1\n",
    "                        M += 1\n",
    "                        self.nkv[z[(idx,n+1)],w] += 1\n",
    "        return M, z\n",
    "\n",
    "    def collapsed_gibbs(self, dtm, maxiter=100):\n",
    "        '''\n",
    "        dtm: document-term matrix of shape [n_docs, n_vocab]\n",
    "        '''\n",
    "        M, z = self.priors(dtm)\n",
    "        for iterr in range(maxiter):\n",
    "            for d in range(self.num_docs):\n",
    "                for i, w in enumerate(DocTermMatrix.word_index(dtm[d,:])):\n",
    "                    self.update_counts(True, d, i, w)\n",
    "                    z[(d,i)] = self.sample_topic_assignment(d, w, M)\n",
    "                    self.update_counts(False, d, i, w)\n",
    "        # can pick a sample that maximises joint probability\n",
    "        # and recover estimates of theta and beta from there\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "    def sample_topic_assignment(self, d, w, M):\n",
    "        # THETA and BETA have been integrated out and so do not enter as parameters.\n",
    "\n",
    "        zdn_probs = np.zeros(self.k)\n",
    "        for k in range(self.k):\n",
    "            # self.nkv.shape[1] = column dimension of self.nkv = vocab_size\n",
    "            prior = (self.local_count[d, k] + (self.alpha/(self.gamma+M))*(self.global_count[k] + self.gamma/K))\n",
    "            prior /= (self.alpha + sum(self.local_count[d,:]))\n",
    "            likelihood = (self.nkv[k,w] + self.eta/self.vocab_size) / (self.nkv[k, :].sum() + self.eta)\n",
    "\n",
    "            zdi_probs[k] = prior * likelihood\n",
    "        # normalize probabilities\n",
    "        zdn_probs /= sum(zdi_probs)\n",
    "        zdn = multinomial(1, zdn_probs).nonzero()[0][0]\n",
    "        return zdn\n",
    "\n",
    "    def update_counts(self, decrease_count=True, d, i, w):\n",
    "        \"\"\"\n",
    "        decrease_count: Boolean\n",
    "        d: \t\t \t\tdocument number\n",
    "        i:\t\t \t\tith word [unordered] in document d\n",
    "        w:\t\t \t\tterm w in document d\n",
    "\n",
    "        Updates count variables to run collapsed sampling equation for LDA.\n",
    "        \"\"\"\n",
    "        # We no longer decrement counts of N_{d,k}?\n",
    "        if decrease_count:\n",
    "            #self.local_count[d,z[(d,i)]] -= 1\n",
    "            self.nkv[z[(d,i)],w] -= 1\n",
    "        else:\n",
    "            #self.local_count[d,self.z[(d,i)]] += 1\n",
    "            self.nkv[z[(d,i)],w] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def roll_die_first_doc(n_d=local_count, n_g=global_count, n=n, M=M):\n",
    "        # probabilities of picking one of s\n",
    "        probs_old_draws = local_count[0, :] / (self.alpha + n)\n",
    "\n",
    "        # probabilities of a new draw being set to s\n",
    "        probs_new_draw_s = global_count[0,:] / (self.gamma + n)\n",
    "        probs_new_draw_s *= (self.alpha / (self.alpha + n))\n",
    "\n",
    "        # probability of a new draw from the base measure\n",
    "        prob_new_from_base = (self.alpha / (self.alpha + M)) * (self.gamma / (self.gamma + M))\n",
    "        \n",
    "        probabilities = list(probs_old_draws) + list(probs_new_draw_s) + list(prob_new_from_base)\n",
    "        draw = choice(len(probabilities), p=probabilities)\n",
    "\n",
    "        return draw\n",
    "\n",
    "    @staticmethod\n",
    "    def roll_die_first_word(n_d=local_count, n_g=global_count, M=M, doc_idx = idx):\n",
    "        # probabilities of picking one of s\n",
    "        probs_old_draws = global_count[idx, :] / (self.gamma + M)\n",
    "\n",
    "        # probability of a new draw\n",
    "        prob_new_draw = self.gamma / (M + self.gamma)\n",
    "\n",
    "        probabilities = list(probs_old_draws) + list(prob_new_draw)\n",
    "        draw = choice(len(probabilities), p=probabilities)\n",
    "        return draw\n",
    "\n",
    "    @staticmethod\n",
    "    def roll_die_nth_word(n_d=local_count, n_g=global_count, n=n, M=M, doc_idx=idx):\n",
    "        # probabilities of picking one of s\n",
    "        probs_old_draws = local_count[idx, :] / (self.alpha + n)\n",
    "\n",
    "        # probability of a new draw\n",
    "        prob_new_draw = self.alpha / (self.alpha + n)\n",
    "\n",
    "        probabilities = list(probs_old_draws) + list(prob_new_draw)\n",
    "        draw = choice(len(probabilities), p=probabilities)\n",
    "        return draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncollapsed Gibbs\n",
    "\n",
    "Here, I was more confused. Mainly, I haven't fully understood how drawing 'm' from a symmetric Dirichlet prior to drawing theta_d is equivalent to the above. In particular, where does the \"rich get richer\" effect would come from?\n",
    "\n",
    "We draw 'm' in the initialization stage, then draw theta's and then topic assignments to words.\n",
    "\n",
    "In further stages of the chain, I did not quite understand how we update 'm' conditionally on its previous draw:\n",
    "    - In the vanilla LDA, the posterior for theta was Dir(alpha + n_{d, :}) where n_{d, :} is a count vector of topics in document d. How does the drawing of 'm' beforehand affect this?\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _initialize(self, dtm):\n",
    "        \"\"\"\n",
    "        dtm: a DxV document-term matrix\n",
    "        Initialises the count variables N_dk, N_kv, N_k, N_d.\n",
    "            K: topic\n",
    "            D: document\n",
    "            V: term\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_docs, self.vocab_size = dtm.shape\n",
    "\n",
    "        # number of terms/words in document D that have topic allocation K\n",
    "        self.ndk = np.zeros((self.num_docs, self.k))\n",
    "        # number of times topic K allocaition variables generate term/word V\n",
    "        self.nkv = np.zeros((self.k, vocab_size))\n",
    "        # number of terms/words generated by topic K\n",
    "        self.nk = np.zeros(self.k)\n",
    "\n",
    "        # The Dirichlet prior for the document-topic distribution\n",
    "        \n",
    "        m = dirichlet(self.gamma * np.ones(self.k)) # m is corpus-wide\n",
    "        theta = dirichlet(self.alpha * m, size = num_docs)\n",
    "\n",
    "        # The Dirichlet prior for the topic-term distribution\n",
    "        beta = dirichlet(self.eta * np.ones(self.vocab_size), size = self.num_topics)\n",
    "\n",
    "        # Initialize the topic assignment variables as a dictionary\n",
    "            # key corresponds to (d,n): nth word in document d\n",
    "            # value = {0,...,K-1} is the topic assigned to that word\n",
    "        self.z = {}\n",
    "        for d in range(self.num_docs):\n",
    "            for i, w in enumerate(DocTermMatrix.word_index(dtm[d,:])):\n",
    "                # total iterations of i will equal the document length\n",
    "                # the set of w includes all terms in document d\n",
    "                self.z[(d,i)] = multinomial(1, theta[d]).nonzero()[0][0]\n",
    "                self.ndk[d,self.z[(d,i)]] += 1\n",
    "                self.nkv[self.z[(d,i)],w] += 1\n",
    "                self.nk[self.z[(d,i)]] += 1\n",
    "\n",
    "    def run_Gibbs(self, dtm, maxIter=100):\n",
    "        \"\"\"\n",
    "        Uncollapsed Gibbs sampling.\n",
    "\n",
    "        matrix: the document-term matrix.\n",
    "        maxIter: the number of iterations to run.\n",
    "\n",
    "        One could construct predictive distributions for theta and beta from the posterior samples (like we do in the collapsed Gibbs).\n",
    "        \"\"\"\n",
    "        self._initialize(dtm)\n",
    "        theta_posterior = theta \t# DxK matrix\n",
    "        beta_posterior = beta \t\t# KxV matrix\n",
    "        theta_samples = []\n",
    "        beta_samples = []\n",
    "\n",
    "        for iterr in range(maxIter):\n",
    "            for k in range(self.num_topics):\n",
    "                # Posterior for BETA\n",
    "                beta_posterior[k,:] = dirichlet(self.eta * np.ones(self.k) + self.nkv[k,:])\n",
    "            for d in range(self.num_docs):\n",
    "                # Posterior for THETA\n",
    "                theta_posterior[d,:] = dirichlet(self.alpha * m + self.ndk[d,:])\n",
    "            for d in range(self.num_docs):\n",
    "                for i, w in enumerate(DocTermMatrix.word_index(dtm[d,:])):\n",
    "                    self.update_counts(True, d, i, w)\n",
    "                    self.z[(d,i)] = self.sample_topic(d, w, theta_posterior, beta_posterior)\n",
    "                    # update counts\n",
    "                    self.update_counts(False, d, i, w)\n",
    "            # Burn-in and thinning interval set to 10 iterations\n",
    "            if (iterr + 1)%10 == 0:\n",
    "                theta_samples.append(theta_posterior)\n",
    "                beta_samples.append(beta_posterior)\n",
    "            if iterr == maxIter:\n",
    "                theta_samples.append(theta_posterior)\n",
    "                beta_samples.append(beta_posterior)\n",
    "        \n",
    "        #return z, theta_samples, beta_samples, self.ndk, self.nkv, self.nk \n",
    "        return theta_samples, beta_samples\n",
    "    \n",
    "    def sample_topic(self, d, w, theta, beta):\n",
    "        zdi_probs = np.zeros(self.num_topics)\n",
    "        for k in range(self.num_topics):\n",
    "            zdi_probs[k] = theta[d,k]*beta[k,w] / np.dot(theta[d,:],beta[:,w])\n",
    "        zdi_probs /= sum(zdi_probs)\n",
    "        zdi = multinomial(1, zdi_probs).nonzero()[0][0]\n",
    "        return zdi\n",
    "    \n",
    "    def update_counts(self, decrease_count=True, d, i, w):\n",
    "        \"\"\"\n",
    "        decrease_count: Boolean\n",
    "        d: \t\t \t\tdocument number\n",
    "        i:\t\t \t\tith word [unordered] in document d\n",
    "        w:\t\t \t\tterm w in document d\n",
    "\n",
    "        Updates count variables to run collapsed sampling equation for LDA.\n",
    "        \"\"\"\n",
    "        if decrease_count:\n",
    "            self.ndk[d,self.z[(d,i)]] -= 1\n",
    "            self.nkv[self.z[(d,i)],w] -= 1\n",
    "        else:\n",
    "            self.ndk[d,self.z[(d,i)]] += 1\n",
    "            self.nkv[self.z[(d,i)],w] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
