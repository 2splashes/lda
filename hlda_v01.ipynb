{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import dirichlet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "class DocTermMatrix(object):\n",
    "\tdef __init__(self, corpus, **kwargs):\n",
    "\t\t'''\n",
    "\t\tcorpus: a collection of raw documents\n",
    "\t\twith **kwargs you can pass arguments of CountVectorizer\n",
    "\t\tcreates a doc-term matrix instance (as scipy sparse matrix)\n",
    "\t\t'''\n",
    "\t\t\n",
    "\t\tvectorizer = CountVectorizer(**kwargs)\n",
    "\t\tself.num_docs = len(corpus)\n",
    "\t\tself.vocab_size = len(vectorizer.vocabulary_)\n",
    "\t\tself.shape = (self.num_docs, self.vocab_size)\n",
    "    \tself.matrix = vectorizer.fit_transform(docs)\n",
    "\t\n",
    "\tdef dataframe(self):\n",
    "\t\t'''\n",
    "\t\treturns the doc-term matrix as pandas dataframe\n",
    "\t\t'''\n",
    "\t\treturn pd.DataFrame(x1.toarray(), columns = vectorizer.get_feature_names())\n",
    "\n",
    "\tdef length(self):\n",
    "\t\t\"\"\"\n",
    "\t\tCalculates the number of words in each document, i.e. its length.\n",
    "\t\tReturns an array of the same size as the corpus.\n",
    "\t\t\"\"\"\n",
    "\t\t# lengths_list = np.zeros(self.row_dim)\n",
    "\t\tlengths_list = self.df().apply(lambda x: len(x.nonzero()[0]), axis=1)\n",
    "\t\treturn lengths_list\n",
    "\n",
    "\t@classmethod\n",
    "\tdef word_index(cls, doc_row):\n",
    "\t\t\"\"\"\n",
    "\t\tdoc_row: document vector (of size V, vocab-size)\n",
    "\n",
    "\t\tReturns an array of repeated indices/terms for the words in the document;\n",
    "\t\tthe indices/terms are repeated the same number of times as the number of occurrences for the corresponding word;\n",
    "\t\tthe length of the array is thus equal to the document length.\n",
    "\t\t\"\"\"\n",
    "\t\t# doc_row = np.array(doc_row)\n",
    "\t\tfor idx in doc_row.nonzero()[0]:\n",
    "        \tfor i in range(int(doc_row[idx])):\n",
    "            \tyield idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My journey through hLDA:\n",
    "\n",
    "Below, firstly, is my implementation of the hierarchical prior for the collapsed Gibbs sampler, largely following the notation of the notes.\n",
    "\n",
    "I have my questions in the code, but here is an overarching one:\n",
    "    - I can understand how, by implementing the Polya urn scheme with probabilities that reward previously drawn topics (\"rich get richer\"?), we would get a few topics being general and others more specific. It is not obvious to me where we made sure that there is a single, unique topic at the root of every path through the hierarchy. [which is how it is plotted in Blei et al. 2004 - 'Hierarchical Topic Models and the Nested Chinese Restaurant Process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hLDA(object):\n",
    "\tdef __init__(self, tree_depth, alpha, gamma, eta):\n",
    "\t\t'''\n",
    "\t\tgamma: the parameter for the (base) Dirichlet from which the prior vector ('m') is drawn.\n",
    "\t\t\t\tin the notes' notation, this is alpha'.\n",
    "\t\ttree_depth: a parameter for the number of levels in a hierarchy.\n",
    "\t\t'''\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.k = tree_depth\n",
    "\t\tself.eta = eta\n",
    "\n",
    "\n",
    "\tdef priors(self, dtm, seed=None):\n",
    "\t\tself.num_docs, self.vocab_size = dtm.shape\n",
    "\n",
    "\t\t# number of times topic K allocaition variables generate term/word V\n",
    "\t\tself.nkv = np.zeros((self.num_topics, self.vocab_size))\n",
    "\n",
    "\t\t#### dtm = DocTermMatrix(corpus).dataframe()\n",
    "\t\t# Document-specific vectors that record the multiplicity of each label\n",
    "\t\tself.local_count = np.zeros((self.num_docs, self.k))\n",
    "\t\t# Global vector which keeps tracks of all draws from the base measure\n",
    "\t\tself.global_count = np.zeros(self.k)\n",
    "\n",
    "\t\tif seed is not None:\n",
    "\t\t\tnp.random.seed(seed)\n",
    "\n",
    "\t\t# Simulating from the hierarchical prior for the first document\n",
    "\t\tsample = {}\n",
    "\t\tsample[0] = multinomial(1,[1/self.k]*self.k).nonzero()[0][0] # sample s1\n",
    "\t\tR = set(sample.values())\n",
    "\t\tM = 1\t\n",
    "\t\tz = {}\n",
    "\t\t# set the topic of the first word in the first doc to s1\n",
    "\t\tz[(0,0)] = sample[0]\n",
    "\t\tself.global_count[0, sample[0]] += 1\n",
    "\t\tself.local_—Åount[0, sample[0]] += 1\n",
    "\t\tfor n, w in enumerate(DocTermMatrix.word_index(dtm[0,:])):\n",
    "\t\t\tdraw = hLDA.roll_die_first_doc()\n",
    "\t\t\tif draw is not None and draw <= self.k:\n",
    "\t\t\t\tz[(0,n+1)] = draw\n",
    "\t\t\t\tself.local_count[0, draw] += 1\n",
    "\t\t\telif draw > self.k and draw <= 2*self.k:\n",
    "\t\t\t\tsample[M+1] = draw - self.k # assign the s_{Mn+1} = s\n",
    "\n",
    "\t\t\t\t# update local and global count?\n",
    "\t\t\t\tself.local_count[0, sample[M+1]] += 1\n",
    "\t\t\t\tself.global_count[0, sample[M+1]] += 1\n",
    "\n",
    "\t\t\t\tM += 1\n",
    "\t\t\t\t# set z_{n+1} equal to?\n",
    "\t\t\t\tz[(0, n+1)] = sample[M+1]\n",
    "\t\t\telif draw > 2*self.k:\n",
    "\t\t\t\tnew_s = multinomial(1,[1/self.k]*self.k).nonzero()[0][0]\n",
    "\t\t\t\tsample[M+1] = new_s\n",
    "\t\t\t\tM += 1\n",
    "\t\t\t\tself.global_count[0, new_s] += 1\n",
    "\t\t\t\tself.local_count[0, new_s] += 1\n",
    "\t\t\t\t# set z_{n+1} equal to?\n",
    "\t\t\t\tz[(0, n+1)] = new_s\n",
    "\t\t\tR = set(sample)\n",
    "\n",
    "\t\t# hierarchical prior across documnents\n",
    "\t\tfor idx, bow in dtm[1:].iterrows(): # bow = bag-of-words\n",
    "\t\t\tfor n, w in enumerate(DocTermMatrix.word_index(bow)):\n",
    "\t\t\t\tif n == 0:\n",
    "\t\t\t\t\tdraw = hLDA.roll_die_first_word()\n",
    "\t\t\t\t\tif draw <= self.k:\n",
    "\t\t\t\t\t\tz[(idx,n)] = draw # double-check\n",
    "\t\t\t\t\t\tself.local_count[idx, draw] = 1\n",
    "\t\t# do we increment counts for M_{k,v} here too? how does this affect the prior of beta vs. usual case?\n",
    "\t\t\t\t\t\t#self.nkv[z[(idx,n)],w] += 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsample[M+1] = multinomial(1,[1/self.k]*self.k).nonzero()[0][0]\n",
    "\t\t\t\t\t\tz[(idx,n)] = sample[M+1] # is this the s in the notes?\n",
    "\t\t\t\t\t\tM += 1\n",
    "\t\t\t\t\t\t#self.nkv[z[(d,i)],w] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdraw = hLDA.roll_die_nth_word()\n",
    "\t\t\t\t\tif draw <= self.k:\n",
    "\t\t\t\t\t\t# need to align n's\n",
    "\t\t\t\t\t\tz[(idx, n+1)] = draw\n",
    "\t\t\t\t\t\tself.global_count[idx, draw] += 1\n",
    "\t\t\t\t\t\tself.local_count[idx, draw] += 1\n",
    "\t\t\t\t\t\t#self.nkv[z[(idx,n+1)],w] += 1\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsample[M+1] = multinomial(1,[1/self.k]*self.k).nonzero()[0][0]\n",
    "\t\t\t\t\t\tz[(idx,n+1)] = sample[M+1]\n",
    "\t\t\t\t\t\tself.global_count[idx, sample[M+1]] += 1\n",
    "\t\t\t\t\t\tself.local_count[idx, sample[M+1]] += 1\n",
    "\t\t\t\t\t\tM += 1\n",
    "\t\t\t\t\t\t#self.nkv[z[(idx,n+1)],w] += 1\n",
    "\t\treturn M, z\n",
    "\n",
    "\tdef collapsed_gibbs(self, dtm, maxiter=100):\n",
    "\t\t'''\n",
    "\t\tdtm: document-term matrix of shape [n_docs, n_vocab]\n",
    "\t\t'''\n",
    "\t\tM, z = self.priors(dtm)\n",
    "\t\tfor iterr in range(maxiter):\n",
    "\t\t\tfor d in range(self.num_docs):\n",
    "\t\t\t\tfor i, w in enumerate(DocTermMatrix.word_index(dtm[d,:])):\n",
    "\t\t\t\t\tself.update_counts(True, d, i, w)\n",
    "\t\t\t\t\tz[(d,i)] = self.sample_topic_assignment(d, w, M)\n",
    "\t\t\t\t\tself.update_counts(False, d, i, w)\n",
    "\t\t# can pick a sample that maximises joint probability\n",
    "\t\t# and recover estimates of theta and beta from there\n",
    "\t\t\n",
    "\t\treturn z\n",
    "\n",
    "\n",
    "\tdef sample_topic_assignment(self, d, w, M):\n",
    "\t\t# THETA and BETA have been integrated out and so do not enter as parameters.\n",
    "\t\t\n",
    "\t\tzdn_probs = np.zeros(self.k)\n",
    "\t\tfor k in range(self.k):\n",
    "\t\t\t# self.nkv.shape[1] = column dimension of self.nkv = vocab_size\n",
    "\t\t\tprior = (self.local_count[d, k] + (self.alpha/(self.gamma+M))*(self.global_count[k] + self.gamma/K))\n",
    "\t\t\tprior /= (self.alpha + sum(self.local_count[d,:]))\n",
    "\t\t\tlikelihood = (self.nkv[k,w] + self.eta/self.vocab_size) / (self.nkv[k, :].sum() + self.eta)\n",
    "\n",
    "\t\t\tzdi_probs[k] = prior * likelihood\n",
    "\t\t# normalize probabilities\n",
    "\t\tzdn_probs /= sum(zdi_probs)\n",
    "\t\tzdn = multinomial(1, zdn_probs).nonzero()[0][0]\n",
    "\t\treturn zdn\n",
    "\n",
    "\tdef update_counts(self, decrease_count=True, d, i, w):\n",
    "\t\t\"\"\"\n",
    "\t\tdecrease_count: Boolean\n",
    "\t\td: \t\t \t\tdocument number\n",
    "\t\ti:\t\t \t\tith word [unordered] in document d\n",
    "\t\tw:\t\t \t\tterm w in document d\n",
    "\n",
    "\t\tUpdates count variables to run collapsed sampling equation for LDA.\n",
    "\t\t\"\"\"\n",
    "\t\t# We no longer decrement counts of N_{d,k}?\n",
    "\t\tif decrease_count:\n",
    "\t\t\t#self.local_count[d,z[(d,i)]] -= 1\n",
    "\t\t\tself.nkv[z[(d,i)],w] -= 1\n",
    "\t\telse:\n",
    "\t\t\t#self.local_count[d,self.z[(d,i)]] += 1\n",
    "\t\t\tself.nkv[self.z[(d,i)],w] += 1\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef roll_die_first_doc(n_d=local_count, n_g=global_count, n=n, M=M):\n",
    "\t\t# probabilities of picking one of s\n",
    "\t\tprobs_old_draws = local_count[0, :] / (self.alpha + n)\n",
    "\n",
    "\t\t# probabilities of a new draw being set to s\n",
    "\t\tprobs_new_draw_s = global_count[0,:] / (self.gamma + n)\n",
    "\t\tprobs_new_draw_s *= (self.alpha / (self.alpha + n))\n",
    "\n",
    "\t\t# probability of a new draw from the base measure\n",
    "\t\tprob_new_from_base = (self.alpha / (self.alpha + M)) * (self.gamma / (self.gamma + M))\n",
    "\t\t\n",
    "\t\tprobabilities = list(probs_old_draws) + list(probs_new_draw_s) + list(prob_new_from_base)\n",
    "\t\tdraw = choice(len(probabilities), p=probabilities)\n",
    "\n",
    "\t\treturn draw\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef roll_die_first_word(n_d=local_count, n_g=global_count, M=M, doc_idx = idx):\n",
    "\t\t# probabilities of picking one of s\n",
    "\t\tprobs_old_draws = global_count[idx, :] / (self.gamma + M)\n",
    "\n",
    "\t\t# probability of a new draw\n",
    "\t\tprob_new_draw = self.gamma / (M + self.gamma)\n",
    "\n",
    "\t\tprobabilities = list(probs_old_draws) + list(prob_new_draw)\n",
    "\t\tdraw = choice(len(probabilities), p=probabilities)\n",
    "\t\treturn draw\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef roll_die_nth_word(n_d=local_count, n_g=global_count, n=n, M=M, doc_idx=idx):\n",
    "\t\t# probabilities of picking one of s\n",
    "\t\tprobs_old_draws = local_count[idx, :] / (self.alpha + n)\n",
    "\n",
    "\t\t# probability of a new draw\n",
    "\t\tprob_new_draw = self.alpha / (self.alpha + n)\n",
    "\n",
    "\t\tprobabilities = list(probs_old_draws) + list(prob_new_draw)\n",
    "\t\tdraw = choice(len(probabilities), p=probabilities)\n",
    "\t\treturn draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncollapsed Gibbs\n",
    "\n",
    "Here, I was more confused. Mainly, I haven't fully understood how drawing 'm' from a symmetric Dirichlet prior to drawing theta_d is equivalent to the above. In particular, where does the \"rich get richer\" effect would come from?\n",
    "\n",
    "We draw 'm' in the initialization stage, then draw theta's and then topic assignments to words.\n",
    "\n",
    "In further stages of the chain, I did not quite understand how we update 'm' conditionally on its previous draw:\n",
    "    - In the vanilla LDA, the conditional posterior for theta was Dir(alpha + n_{d, :}) where n_{d, :} is a count vector of topics in document d. How does the drawing of 'm' beforehand affect this?\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _initialize(self, dtm):\n",
    "\t\t\"\"\"\n",
    "\t\tdtm: a DxV document-term matrix\n",
    "\n",
    "\t\tInitialises the count variables N_dk, N_kv, N_k, N_d.\n",
    "\t\t\tK: topic\n",
    "\t\t\tD: document\n",
    "\t\t\tV: term\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.num_docs, self.vocab_size = dtm.shape\n",
    "\n",
    "\t\t# number of terms/words in document D that have topic allocation K\n",
    "\t\tself.ndk = np.zeros((self.num_docs, self.k))\n",
    "\t\t# number of times topic K allocaition variables generate term/word V\n",
    "\t\tself.nkv = np.zeros((self.k, vocab_size))\n",
    "\t\t# number of terms/words generated by topic K\n",
    "\t\tself.nk = np.zeros(self.k)\n",
    "\n",
    "\t\t# The Dirichlet prior for the document-topic distribution\n",
    "        \n",
    "        m = dirichlet(self.gamma * np.ones(self.k)) # m is corpus-wide\n",
    "\t\ttheta = dirichlet(self.alpha * m, size = num_docs)\n",
    "\n",
    "\t\t# The Dirichlet prior for the topic-term distribution\n",
    "\t\tbeta = dirichlet(self.eta * np.ones(self.vocab_size), size = self.num_topics)\n",
    "\n",
    "\t\t# Initialize the topic assignment variables as a dictionary\n",
    "\t\t\t# key corresponds to (d,n): nth word in document d\n",
    "\t\t\t# value = {0,...,K-1} is the topic assigned to that word\n",
    "\t\tself.z = {}\n",
    "\t\tfor d in range(self.num_docs):\n",
    "\t\t\tfor i, w in enumerate(DocTermMatrix.word_index(dtm[d,:])):\n",
    "\t\t\t\t# total iterations of i will equal the document length\n",
    "\t\t\t\t# the set of w includes all terms in document d\n",
    "\t\t\t\tself.z[(d,i)] = multinomial(1, theta[d]).nonzero()[0][0]\n",
    "\t\t\t\tself.ndk[d,self.z[(d,i)]] += 1\n",
    "\t\t\t\tself.nkv[self.z[(d,i)],w] += 1\n",
    "\t\t\t\tself.nk[self.z[(d,i)]] += 1\n",
    "\n",
    "\tdef run_Gibbs(self, dtm, maxIter=100):\n",
    "\t\t\"\"\"\n",
    "\t\tUncollapsed Gibbs sampling.\n",
    "\n",
    "\t\tmatrix: the document-term matrix.\n",
    "\t\tmaxIter: the number of iterations to run.\n",
    "\n",
    "\t\tOne could construct predictive distributions for theta and beta from the posterior samples (like we do in the collapsed Gibbs).\n",
    "\t\t\"\"\"\n",
    "\t\tself._initialize(dtm)\n",
    "\t\ttheta_posterior = theta \t# DxK matrix\n",
    "\t\tbeta_posterior = beta \t\t# KxV matrix\n",
    "\t\ttheta_samples = []\n",
    "\t\tbeta_samples = []\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# The number of loops seems very inefficient\n",
    "\t\tfor iterr in range(maxIter):\n",
    "\t\t\tfor k in range(self.num_topics):\n",
    "\t\t\t\t# Posterior for BETA\n",
    "\t\t\t\tbeta_posterior[k,:] = dirichlet(self.eta * np.ones(self.k) + self.nkv[k,:])\n",
    "\t\t\tfor d in range(self.num_docs):\n",
    "\t\t\t\t# Posterior for THETA\n",
    "\t\t\t\ttheta_posterior[d,:] = dirichlet(self.alpha * m + self.ndk[d,:])\n",
    "\t\t\tfor d in range(self.num_docs):\n",
    "\t\t\t\tfor i, w in enumerate(DocTermMatrix.word_index(dtm[d,:])):\n",
    "\t\t\t\t\tself.update_counts(True, d, i, w)\n",
    "\t\t\t\t\tself.z[(d,i)] = self.sample_topic(d, w, theta_posterior, beta_posterior)\n",
    "\t\t\t\t\t# update counts\n",
    "\t\t\t\t\tself.update_counts(False, d, i, w)\n",
    "\t\t\t# Burn-in and thinning interval set to 10 iterations\n",
    "\t\t\tif (iterr + 1)%10 == 0:\n",
    "\t\t\t\ttheta_samples.append(theta_posterior)\n",
    "\t\t\t\tbeta_samples.append(beta_posterior)\n",
    "\t\t\tif iterr == maxIter:\n",
    "\t\t\t\ttheta_samples.append(theta_posterior)\n",
    "\t\t\t\tbeta_samples.append(beta_posterior)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t#return z, theta_samples, beta_samples, self.ndk, self.nkv, self.nk \n",
    "\t\treturn theta_samples, beta_samples\n",
    "    \n",
    "\tdef sample_topic(self, d, w, theta, beta):\n",
    "\t\tzdi_probs = np.zeros(self.num_topics)\n",
    "\t\tfor k in range(self.num_topics):\n",
    "\t\t\tzdi_probs[k] = theta[d,k]*beta[k,w] / np.dot(theta[d,:],beta[:,w])\n",
    "\t\tzdi_probs /= sum(zdi_probs)\n",
    "\t\tzdi = multinomial(1, zdi_probs).nonzero()[0][0]\n",
    "\t\treturn zdi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
